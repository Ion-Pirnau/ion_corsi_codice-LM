{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "wlo5TZ0m7Lq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questo laboratorio vedermo come implementare il clustering in Python. A differenza degli altri metodi che abbiamo visto negli scorsi laboratori, il custering è un metodo di learning unsupervised, cioè non vengono usate le etichette di classe"
      ],
      "metadata": {
        "id": "5Hanxx5T7POE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installiamo:\n",
        "#1) scprep, ossia un framework  per caricare, preelaborare e disegnare matrici in Python;\n",
        "#2) graphtool e louvain per confrontare k-means con altri algoritmi di clustering\n",
        "!pip install scprep graphtools louvain"
      ],
      "metadata": {
        "id": "8uiiFdgAXC7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creiamo il dataset"
      ],
      "metadata": {
        "id": "5tHWPm6U7miL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYVS4Xrd7HXu"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=300, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L'algoritmo K-Means\n",
        "\n",
        "L'algoritmo $K$-means divide un insieme di $N$ campioni $X$ in $K$ cluster disgiunti $C$, ciascuno descritto dalla media $\\mu_j$ dei campioni nel cluster. Le medie sono comunemente chiamate **“centroidi” del cluster**; si noti che non sono, in generale, punti del dataset $X$, sebbene vengano rappresentati nello stesso spazio. L'algoritmo K-means mira a scegliere i centroidi che riducono al minimo la somma dei quadrati all'interno del cluster (inerzia):\n",
        "\n",
        "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_j - \\mu_i||^2)$$\n",
        "\n",
        "## Come funziona l'algoritmo\n",
        "\n",
        "L'algoritmo  K-mean utilizza un meccanismo di perfezionamento iterativo per produrre i cluster finali. Gli input dell'algoritmo sono il numero di cluster $Κ$ e il set di dati. K-Means  inizializza i $Κ$ centroidi iniziali, che possono essere generati casualmente o selezionati casualmente dal set di dati. L'algoritmo quindi ripete due passaggi:\n",
        "\n",
        "**Assegnamento**: ciascun centroide definisce uno dei cluster. In questo passaggio, ciascun punto viene assegnato al centroide più vicino, in base alla distanza euclidea. Più formalmente, se $c_i$ è l'insieme dei centroidi nell'insieme $C$, allora ogni punto dati $x$ viene assegnato a un cluster in base al seguente criterio:\n",
        "\n",
        "$$\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2$$\n",
        "\n",
        "dove dist( · ) è la distanza euclidea standard ($L_2$).\n",
        "\n",
        "**Aggiornamento dei centroidi**: in questo step i centroidi vengono ricalcolati. Sia $S_i$ l'insieme degli assegnamenti dei punti rispetto a ciascun per centroide del cluster.L'aggiornamento avviene stiamndo la media di tutti i punti  assegnati al medesimo cluster.\n",
        "\n",
        "$$c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i x_i}$$\n",
        "\n",
        "L'algoritmo ripete i due step fino a convergenza, ossia nessun punto modifica il suo assegnamento tra iterazioni successive (la somma delle distanze è ridotta al minimo) o viene raggiunto un numero massimo di iterazioni.\n",
        "\n",
        "**Convergenza e inizializzazione casuale**\n",
        "\n",
        "È garantito che questo algoritmo converga verso un punto di minimo. Il risultato può essere un ottimo locale, il che significa che la valutazione di più di un'esecuzione dell'algoritmo con centroidi iniziali randomizzati può fornire un risultato migliore.\n",
        "\n",
        "<img src=https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif style=\"width: 500px;\"/>\n"
      ],
      "metadata": {
        "id": "fFTnHD3ePRsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Come valutare K-means in pratica\n",
        "\n",
        "Applichiamo K-means con un numero di cluster K variabile. Successivamente, per ogni valore di K, andiamo a valutare la somma delle distanze quadrate tra i punti e il centroide del cluster (ovvero quello che in Sklearn si chiama \"inerzia\")."
      ],
      "metadata": {
        "id": "72qZNpcCDIR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'inerzia misura quanto bene un set di dati è stato raggruppato per mezzo dell'algoritmo K-Means. Viene calcolato misurando la distanza tra ciascun punto e il relativo centroide, e sommando le i quadrati delle distanze.\n",
        "\n",
        "$$\\sum_{i=0}^{n}||x_j - \\mu_i||^2$$\n",
        "\n",
        "Un buon modello è quello con bassa inerzia e un ridotto numero di cluster (K). Tuttavia, questo è un compromesso perché all'aumentare di K, l'inerzia diminuisce.\n",
        "\n",
        "Per trovare il K ottimale per un set di dati, si utilizza il metodo Elbow, che consiste nel trovare il punto in cui la diminuzione dell'inerzia inizia a rallentare."
      ],
      "metadata": {
        "id": "GSoDvTs7F7Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Applica K-means con un numero di cluster variabile\n",
        "k_values = range(2, 10)\n",
        "inertias = []\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Mostra i risultati al variare di K con uno scatter plot\n",
        "plt.plot(k_values, inertias, '-o')\n",
        "plt.xlabel('Numero di cluster, K')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia vs. Numero di cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7GjZO0GhCDEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scegliamo k=4 numero di cluster, usando l'elbow method. In sostanza, k=4 è un buon numero di cluster in quanto già con k=5 l'inerzia non diminuisce più di tanto."
      ],
      "metadata": {
        "id": "jpbC3blIDn2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plottiamo ora i punti in uno spazio bidimensionale e coloriamo i punti in base al cluster di appartenenza, con i relativi centroidi in grigio"
      ],
      "metadata": {
        "id": "hGrqFUaXEKPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# Mostra i risultati con uno scatter plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ulmRiZAZDk4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo anche valutare la bontà del clustering usando la Dissimilarity Matrix."
      ],
      "metadata": {
        "id": "-DXwbeN2Emjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otteniamo i centroidi di ciascun cluster**"
      ],
      "metadata": {
        "id": "pzwwl1M_TXN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clus_cent=kmeans.cluster_centers_\n",
        "clus_cent"
      ],
      "metadata": {
        "id": "3LTW77_pTcCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otteniamo gli assegnamenti di ciascun punto**"
      ],
      "metadata": {
        "id": "S48YGZ1cTmkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "JrIyBXB_TxPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "import seaborn as sns\n",
        "\n",
        "distances = pairwise_distances(X)\n",
        "sns.heatmap(distances, cmap='viridis')\n",
        "plt.title('Dissimilarity Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fGK9FhldEVMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster omogenei dovrebbero essere rappresentati come regioni scure sulla diagonale della matrice. Gli elementi della matrice più luminosi rappresentano i punti del dataset che sono più lontani tra loro."
      ],
      "metadata": {
        "id": "HnvrNvfJE91k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valutiamo le capacità di K-Means su diversi distribuzioni di dati"
      ],
      "metadata": {
        "id": "cSAhHrviXfw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.datasets\n",
        "np.random.seed(0)\n",
        "# ============\n",
        "# Generate datasets. We choose the size big enough to see the scalability\n",
        "# of the algorithms, but not too big to avoid too long running times\n",
        "# ============\n",
        "n_samples = 1500\n",
        "\n",
        "# Circles\n",
        "noisy_circles = sklearn.datasets.make_circles(\n",
        "    n_samples=n_samples,\n",
        "    # Scale factor between inner and outer circle\n",
        "    factor=.5,\n",
        "    # Gaussian noise added to each point\n",
        "    noise=.05)\n",
        "\n",
        "# Moons\n",
        "noisy_moons = sklearn.datasets.make_moons(n_samples=n_samples,\n",
        "                                          noise=.05)\n",
        "\n",
        "# Blobs\n",
        "blobs = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=8, cluster_std=1)\n",
        "\n",
        "# Uniform square\n",
        "no_structure = (np.random.uniform(size=(n_samples, 2)), None)\n",
        "\n",
        "# Anisotropically distributed data\n",
        "random_state = 170\n",
        "X, y = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1)\n",
        "# Changes how x1, x2 coordinates are shifted\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "aniso = (X_aniso, y)\n",
        "\n",
        "# blobs with varied variances\n",
        "varied = sklearn.datasets.make_blobs(n_samples=n_samples,\n",
        "                                     cluster_std=[1.0, 2.5, 0.5],\n",
        "                                     random_state=random_state)\n",
        "\n",
        "# ============\n",
        "# Associate each dataset with the correct # of clusters\n",
        "# ============\n",
        "\n",
        "default_base = {'n_clusters': 3}\n",
        "\n",
        "generated_datasets = [\n",
        "    (noisy_circles, {'n_clusters': 2}),\n",
        "    (noisy_moons, {'n_clusters': 2}),\n",
        "    (varied,      {}),\n",
        "    (aniso,       {}),\n",
        "    (blobs, {}),\n",
        "    (no_structure, {})]"
      ],
      "metadata": {
        "id": "WY6pkYyFECtJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizziamo la groud truth**"
      ],
      "metadata": {
        "id": "hNYN2rx4WfFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scprep\n",
        "\n",
        "fig, axes = plt.subplots(1,6,figsize=(12,2))\n",
        "\n",
        "for i, (dataset, _) in enumerate(generated_datasets):\n",
        "    ax = axes[i]\n",
        "    X, y = dataset\n",
        "\n",
        "    # normalize dataset for easier parameter selection\n",
        "    X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n",
        "    scprep.plot.scatter2d(X, c=y,\n",
        "                          ticks=None, ax=ax,\n",
        "                          xlabel='x0', ylabel='x1',\n",
        "                         legend=False)\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "YflBfX-JWkP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import louvain\n",
        "import graphtools as gt\n",
        "\n",
        "fig, axes = plt.subplots(6,3, figsize=(12, 20))\n",
        "plot_title = True\n",
        "\n",
        "for i_dataset, (dataset, cluster_params) in enumerate(generated_datasets):\n",
        "    # update cluster parameters with dataset-specific values\n",
        "    params = default_base.copy()\n",
        "    params.update(cluster_params)\n",
        "\n",
        "    X, y = dataset\n",
        "\n",
        "    # normalize dataset for easier parameter selection\n",
        "    X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n",
        "\n",
        "\n",
        "    # ============\n",
        "    # Run clustering algorithms\n",
        "    # ============\n",
        "    clusters = []\n",
        "    titles = []\n",
        "    times = []\n",
        "    # KMeans\n",
        "    tic = time.time()\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=params['n_clusters'])\n",
        "    clusters.append(kmeans.fit_predict(X))\n",
        "    titles.append('KMeans')\n",
        "    times.append(time.time() - tic)\n",
        "\n",
        "    # Spectral Clustering\n",
        "    tic = time.time()\n",
        "    spectral = sklearn.cluster.SpectralClustering(\n",
        "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
        "        affinity=\"nearest_neighbors\")\n",
        "    clusters.append(spectral.fit_predict(X))\n",
        "    titles.append('Spectral')\n",
        "    times.append(time.time() - tic)\n",
        "\n",
        "\n",
        "    # Louvain\n",
        "    tic = time.time()\n",
        "    G = gt.Graph(X)\n",
        "    G_igraph = G.to_igraph()\n",
        "    part = louvain.find_partition(G_igraph, louvain.RBConfigurationVertexPartition,\n",
        "                                  weights=\"weight\", resolution_parameter=0.01)\n",
        "    clusters.append(np.array(part.membership))\n",
        "    titles.append('Louvain')\n",
        "    times.append(time.time() - tic)\n",
        "\n",
        "    # ============\n",
        "    # Plot clustering results for dataset\n",
        "    # ============\n",
        "    row_axes = axes[i_dataset]\n",
        "\n",
        "    for i, ax in enumerate(row_axes.flatten()):\n",
        "        curr_cluster = clusters[i]\n",
        "        if plot_title:\n",
        "            curr_title = '{}'.format(titles[i])\n",
        "        else:\n",
        "            curr_title = None\n",
        "\n",
        "        scprep.plot.scatter2d(X, c=curr_cluster, title=curr_title, ax=ax,\n",
        "                             legend=False, discrete=True)\n",
        "\n",
        "        # Plot time to run algorithm\n",
        "        plt.text(.99, .01, ('%.2fs' % (times[i])).lstrip('0'),\n",
        "                 transform=ax.transAxes, size=15,\n",
        "                 horizontalalignment='right')\n",
        "    plot_title=False\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "NHPgW86nXnMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Silhouette\n",
        "Oltre all'inerzia, che però è soggetta a diminuire in funzione dell'aumentare del numero di cluster, per validare un metodo di clustering si possono solitamente utilizzare due criteri: la **distanza intercluster** e la **distanza intracluster**.\n",
        "\n",
        "E di conseguenza unire il tutto all'interno della misura Silhouette:\n",
        "$$Silhouette(x) = \\frac{b(x)-a(x)}{max(b(x),a(x))}$$\n",
        "dove\n",
        "- a(x) è la distanza media tra x e tutti gli altri punti all'interno del cluster\n",
        "- b(x) è il minimo delle distanze medie tra x e i punti negli altri cluster\n",
        "\n",
        "\n",
        "<img src=https://exeter-data-analytics.github.io/MachineLearning/_img/02-clustering.png style=\"width: 500px;\"/>\n"
      ],
      "metadata": {
        "id": "bjsK36IKkLnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "X, y_true = make_blobs(n_samples=300, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\n",
        "        \"For n_clusters =\",\n",
        "        n_clusters,\n",
        "        \"The average silhouette_score is :\",\n",
        "        silhouette_avg,\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(\n",
        "            np.arange(y_lower, y_upper),\n",
        "            0,\n",
        "            ith_cluster_silhouette_values,\n",
        "            facecolor=color,\n",
        "            edgecolor=color,\n",
        "            alpha=0.7,\n",
        "        )\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(\n",
        "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
        "    )\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(\n",
        "        centers[:, 0],\n",
        "        centers[:, 1],\n",
        "        marker=\"o\",\n",
        "        c=\"white\",\n",
        "        alpha=1,\n",
        "        s=200,\n",
        "        edgecolor=\"k\",\n",
        "    )\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle(\n",
        "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
        "        % n_clusters,\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lTeeHVNBeTgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "\n",
        "Verificare le capacità di k-Means di effettuare un buon clustering sul dataset Iris, valutando le performance rispetto alla ground truth."
      ],
      "metadata": {
        "id": "UJ-mXri33ZeK"
      }
    }
  ]
}